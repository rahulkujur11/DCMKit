
# ğŸ”¥ Kafka-Based ETL Pipeline for Material Demand & Capacity

A scalable, schema-driven data pipeline built with **FastAPI**, **Kafka**, and **Apache Spark** to validate, store, and forward supply chain data to external APIs.

---

## ğŸ“¦ Project Structure

```
project/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ schema/
â”‚       â”œâ”€â”€ IdBasedComment-schema.json
â”‚       â”œâ”€â”€ WeekBasedMaterialDemand-schema.json
â”‚       â””â”€â”€ WeekBasedCapacityGroup-schema.json
â”œâ”€â”€ transforms/
â”‚   â”œâ”€â”€ transform_comment.py
â”‚   â”œâ”€â”€ transform_demand.py
â”‚   â””â”€â”€ transform_capacity.py
â”œâ”€â”€ logs/
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ comments-topic/
â”‚   â”œâ”€â”€ demand-topic/
â”‚   â””â”€â”€ capacity-topic/
â”œâ”€â”€ main.py                # FastAPI server
â”œâ”€â”€ consumer.py            # Kafka raw message archiver
â”œâ”€â”€ spark_processor.py     # Spark Kafka stream processor
â”œâ”€â”€ config.py              # Settings and paths
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Tech Stack

- **FastAPI** â€“ JSON schema validation & REST API
- **Apache Kafka** â€“ Message streaming backbone
- **Apache Spark (Structured Streaming)** â€“ Real-time ETL & API forwarding
- **JSONSchema (Draft-07)** â€“ Schema validation
- **Tenacity** â€“ Retry logic for failed API posts
- **Logging** â€“ Traceable logs across all components

---

## ğŸ› ï¸ Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Include dependencies like:
```txt
fastapi
uvicorn
jsonschema
kafka-python
pyspark
tenacity
requests
```

### 2. Start Kafka

Ensure Kafka is running at `localhost:9092`.

### 3. Launch FastAPI Server

```bash
uvicorn main:app --reload
```

### 4. Start Kafka Consumer

This stores raw validated messages to disk.

```bash
python consumer.py
```

### 5. Start Spark Processor

```bash
python spark_processor.py
```

---

## ğŸŒ API Endpoints

| Endpoint     | Description                         | Kafka Topic      |
|--------------|-------------------------------------|------------------|
| `POST /comments` | Accepts user comment payloads       | `comments-topic` |
| `POST /demand`   | Accepts week-based material demand | `demand-topic`   |
| `POST /capacity` | Accepts weekly capacity info       | `capacity-topic` |

Each endpoint validates payloads using its schema in `/api/schema`.

---

## ğŸ“¤ Output Data

### ğŸ§¾ Raw JSON Archive
- Saved in `raw_data/<topic-name>/`
- Filename pattern: `YYYYMMDDTHHMMSS_UUID.json`

### ğŸ” External API Posting

After transformation, payloads are posted to:

| Kafka Topic      | Target API URL                                  |
|------------------|--------------------------------------------------|
| `comments-topic` | `https://myo9poc.o9solutions.com/api/pulse`      |
| `demand-topic`   | `https://myo9poc.o9solutions.com/api/demand`     |
| `capacity-topic` | `https://myo9poc.o9solutions.com/api/capacity`   |

Includes:
- Chunked posting (`100` records per batch)
- Retry logic (5 attempts, exponential backoff)

---

## ğŸ§ª Testing & Debugging

- Use Postman or `curl` to hit API endpoints with valid/invalid payloads.
- Check:
  - FastAPI logs for validation errors
  - Consumer logs for file save issues
  - Spark logs for transformation/posting problems

---

## ğŸ§° Dev Tips

- âœ… Use `.trigger(once=True)` in Spark for batch testing
- ğŸš§ Add more schema files under `api/schema/` for future endpoints
- ğŸ”’ Secure API keys using `.env` or `AWS Secrets Manager`

---

## ğŸ” Security Note

Hardcoded API keys (`API_KEY = "..."`) should be **moved to secure storage** before production deployment.

---

## ğŸ‘¨â€ğŸ”§ Contributors

- Backend + ETL Orchestration: You
- API Design & Schema Validation: Also You
- Sanity Preserved by: Coffee

---

## ğŸ“œ License

MIT â€“ do what you want, but don't blame us if it breaks.

---
# DCMKit
